{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP Sonar 12 neurones .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPXxfka0t+L6ylBWT5dbTlU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BalkisG77/pythoncode-tutorials/blob/master/TP_Sonar_12_neurones_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q40nX7Z2juzC"
      },
      "source": [
        "import pandas as pd\n",
        "from tensorflow.python.ops.gen_data_flow_ops import tensor_array"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "kFpdOOk4j1cT",
        "outputId": "1ad22354-09c3-4d08-9ee6-aeacb6e59905"
      },
      "source": [
        "sonar = pd.read_csv(\"/content/sonar.all-data.csv\", header = None)\n",
        "sonar.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0200</td>\n",
              "      <td>0.0371</td>\n",
              "      <td>0.0428</td>\n",
              "      <td>0.0207</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>0.0986</td>\n",
              "      <td>0.1539</td>\n",
              "      <td>0.1601</td>\n",
              "      <td>0.3109</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>0.1609</td>\n",
              "      <td>0.1582</td>\n",
              "      <td>0.2238</td>\n",
              "      <td>0.0645</td>\n",
              "      <td>0.0660</td>\n",
              "      <td>0.2273</td>\n",
              "      <td>0.3100</td>\n",
              "      <td>0.2999</td>\n",
              "      <td>0.5078</td>\n",
              "      <td>0.4797</td>\n",
              "      <td>0.5783</td>\n",
              "      <td>0.5071</td>\n",
              "      <td>0.4328</td>\n",
              "      <td>0.5550</td>\n",
              "      <td>0.6711</td>\n",
              "      <td>0.6415</td>\n",
              "      <td>0.7104</td>\n",
              "      <td>0.8080</td>\n",
              "      <td>0.6791</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>0.1307</td>\n",
              "      <td>0.2604</td>\n",
              "      <td>0.5121</td>\n",
              "      <td>0.7547</td>\n",
              "      <td>0.8537</td>\n",
              "      <td>0.8507</td>\n",
              "      <td>0.6692</td>\n",
              "      <td>0.6097</td>\n",
              "      <td>0.4943</td>\n",
              "      <td>0.2744</td>\n",
              "      <td>0.0510</td>\n",
              "      <td>0.2834</td>\n",
              "      <td>0.2825</td>\n",
              "      <td>0.4256</td>\n",
              "      <td>0.2641</td>\n",
              "      <td>0.1386</td>\n",
              "      <td>0.1051</td>\n",
              "      <td>0.1343</td>\n",
              "      <td>0.0383</td>\n",
              "      <td>0.0324</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>0.0159</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0167</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0453</td>\n",
              "      <td>0.0523</td>\n",
              "      <td>0.0843</td>\n",
              "      <td>0.0689</td>\n",
              "      <td>0.1183</td>\n",
              "      <td>0.2583</td>\n",
              "      <td>0.2156</td>\n",
              "      <td>0.3481</td>\n",
              "      <td>0.3337</td>\n",
              "      <td>0.2872</td>\n",
              "      <td>0.4918</td>\n",
              "      <td>0.6552</td>\n",
              "      <td>0.6919</td>\n",
              "      <td>0.7797</td>\n",
              "      <td>0.7464</td>\n",
              "      <td>0.9444</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8874</td>\n",
              "      <td>0.8024</td>\n",
              "      <td>0.7818</td>\n",
              "      <td>0.5212</td>\n",
              "      <td>0.4052</td>\n",
              "      <td>0.3957</td>\n",
              "      <td>0.3914</td>\n",
              "      <td>0.3250</td>\n",
              "      <td>0.3200</td>\n",
              "      <td>0.3271</td>\n",
              "      <td>0.2767</td>\n",
              "      <td>0.4423</td>\n",
              "      <td>0.2028</td>\n",
              "      <td>0.3788</td>\n",
              "      <td>0.2947</td>\n",
              "      <td>0.1984</td>\n",
              "      <td>0.2341</td>\n",
              "      <td>0.1306</td>\n",
              "      <td>0.4182</td>\n",
              "      <td>0.3835</td>\n",
              "      <td>0.1057</td>\n",
              "      <td>0.1840</td>\n",
              "      <td>0.1970</td>\n",
              "      <td>0.1674</td>\n",
              "      <td>0.0583</td>\n",
              "      <td>0.1401</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.0621</td>\n",
              "      <td>0.0203</td>\n",
              "      <td>0.0530</td>\n",
              "      <td>0.0742</td>\n",
              "      <td>0.0409</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0125</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>0.0191</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0262</td>\n",
              "      <td>0.0582</td>\n",
              "      <td>0.1099</td>\n",
              "      <td>0.1083</td>\n",
              "      <td>0.0974</td>\n",
              "      <td>0.2280</td>\n",
              "      <td>0.2431</td>\n",
              "      <td>0.3771</td>\n",
              "      <td>0.5598</td>\n",
              "      <td>0.6194</td>\n",
              "      <td>0.6333</td>\n",
              "      <td>0.7060</td>\n",
              "      <td>0.5544</td>\n",
              "      <td>0.5320</td>\n",
              "      <td>0.6479</td>\n",
              "      <td>0.6931</td>\n",
              "      <td>0.6759</td>\n",
              "      <td>0.7551</td>\n",
              "      <td>0.8929</td>\n",
              "      <td>0.8619</td>\n",
              "      <td>0.7974</td>\n",
              "      <td>0.6737</td>\n",
              "      <td>0.4293</td>\n",
              "      <td>0.3648</td>\n",
              "      <td>0.5331</td>\n",
              "      <td>0.2413</td>\n",
              "      <td>0.5070</td>\n",
              "      <td>0.8533</td>\n",
              "      <td>0.6036</td>\n",
              "      <td>0.8514</td>\n",
              "      <td>0.8512</td>\n",
              "      <td>0.5045</td>\n",
              "      <td>0.1862</td>\n",
              "      <td>0.2709</td>\n",
              "      <td>0.4232</td>\n",
              "      <td>0.3043</td>\n",
              "      <td>0.6116</td>\n",
              "      <td>0.6756</td>\n",
              "      <td>0.5375</td>\n",
              "      <td>0.4719</td>\n",
              "      <td>0.4647</td>\n",
              "      <td>0.2587</td>\n",
              "      <td>0.2129</td>\n",
              "      <td>0.2222</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>0.0176</td>\n",
              "      <td>0.1348</td>\n",
              "      <td>0.0744</td>\n",
              "      <td>0.0130</td>\n",
              "      <td>0.0106</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0166</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0244</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0164</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0078</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0100</td>\n",
              "      <td>0.0171</td>\n",
              "      <td>0.0623</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.1098</td>\n",
              "      <td>0.1276</td>\n",
              "      <td>0.0598</td>\n",
              "      <td>0.1264</td>\n",
              "      <td>0.0881</td>\n",
              "      <td>0.1992</td>\n",
              "      <td>0.0184</td>\n",
              "      <td>0.2261</td>\n",
              "      <td>0.1729</td>\n",
              "      <td>0.2131</td>\n",
              "      <td>0.0693</td>\n",
              "      <td>0.2281</td>\n",
              "      <td>0.4060</td>\n",
              "      <td>0.3973</td>\n",
              "      <td>0.2741</td>\n",
              "      <td>0.3690</td>\n",
              "      <td>0.5556</td>\n",
              "      <td>0.4846</td>\n",
              "      <td>0.3140</td>\n",
              "      <td>0.5334</td>\n",
              "      <td>0.5256</td>\n",
              "      <td>0.2520</td>\n",
              "      <td>0.2090</td>\n",
              "      <td>0.3559</td>\n",
              "      <td>0.6260</td>\n",
              "      <td>0.7340</td>\n",
              "      <td>0.6120</td>\n",
              "      <td>0.3497</td>\n",
              "      <td>0.3953</td>\n",
              "      <td>0.3012</td>\n",
              "      <td>0.5408</td>\n",
              "      <td>0.8814</td>\n",
              "      <td>0.9857</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>0.6121</td>\n",
              "      <td>0.5006</td>\n",
              "      <td>0.3210</td>\n",
              "      <td>0.3202</td>\n",
              "      <td>0.4295</td>\n",
              "      <td>0.3654</td>\n",
              "      <td>0.2655</td>\n",
              "      <td>0.1576</td>\n",
              "      <td>0.0681</td>\n",
              "      <td>0.0294</td>\n",
              "      <td>0.0241</td>\n",
              "      <td>0.0121</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0150</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0117</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0762</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.0481</td>\n",
              "      <td>0.0394</td>\n",
              "      <td>0.0590</td>\n",
              "      <td>0.0649</td>\n",
              "      <td>0.1209</td>\n",
              "      <td>0.2467</td>\n",
              "      <td>0.3564</td>\n",
              "      <td>0.4459</td>\n",
              "      <td>0.4152</td>\n",
              "      <td>0.3952</td>\n",
              "      <td>0.4256</td>\n",
              "      <td>0.4135</td>\n",
              "      <td>0.4528</td>\n",
              "      <td>0.5326</td>\n",
              "      <td>0.7306</td>\n",
              "      <td>0.6193</td>\n",
              "      <td>0.2032</td>\n",
              "      <td>0.4636</td>\n",
              "      <td>0.4148</td>\n",
              "      <td>0.4292</td>\n",
              "      <td>0.5730</td>\n",
              "      <td>0.5399</td>\n",
              "      <td>0.3161</td>\n",
              "      <td>0.2285</td>\n",
              "      <td>0.6995</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.7262</td>\n",
              "      <td>0.4724</td>\n",
              "      <td>0.5103</td>\n",
              "      <td>0.5459</td>\n",
              "      <td>0.2881</td>\n",
              "      <td>0.0981</td>\n",
              "      <td>0.1951</td>\n",
              "      <td>0.4181</td>\n",
              "      <td>0.4604</td>\n",
              "      <td>0.3217</td>\n",
              "      <td>0.2828</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.1979</td>\n",
              "      <td>0.2444</td>\n",
              "      <td>0.1847</td>\n",
              "      <td>0.0841</td>\n",
              "      <td>0.0692</td>\n",
              "      <td>0.0528</td>\n",
              "      <td>0.0357</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0230</td>\n",
              "      <td>0.0046</td>\n",
              "      <td>0.0156</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0110</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0107</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0       1       2       3       4   ...      56      57      58      59  60\n",
              "0  0.0200  0.0371  0.0428  0.0207  0.0954  ...  0.0180  0.0084  0.0090  0.0032   R\n",
              "1  0.0453  0.0523  0.0843  0.0689  0.1183  ...  0.0140  0.0049  0.0052  0.0044   R\n",
              "2  0.0262  0.0582  0.1099  0.1083  0.0974  ...  0.0316  0.0164  0.0095  0.0078   R\n",
              "3  0.0100  0.0171  0.0623  0.0205  0.0205  ...  0.0050  0.0044  0.0040  0.0117   R\n",
              "4  0.0762  0.0666  0.0481  0.0394  0.0590  ...  0.0072  0.0048  0.0107  0.0094   R\n",
              "\n",
              "[5 rows x 61 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oye4svEPkMuY"
      },
      "source": [
        "PREPARATION DES DONNEES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jA2ZjN2EkQfj",
        "outputId": "155c6177-a44e-4aca-c239-211588fbe0af"
      },
      "source": [
        "print(\"Nbr colonnes: \", len(sonar.columns))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nbr colonnes:  61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbIsF5OEkUZe"
      },
      "source": [
        "#On ne prend que les données issues du sonar pour l'apprentissage\n",
        "X = sonar[range(60)].values"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1HgIFA2kXvc"
      },
      "source": [
        "#On ne prend que les libellé\n",
        "y = sonar[60].values"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th3P7_E2kcpY"
      },
      "source": [
        "#On encode : Les mines sont égales à 0 et les rochers égaux à 1\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(y)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-qGjfYLksWL"
      },
      "source": [
        "#On ajoute un encodage pour créer des classes :\n",
        "# Si c'est une mine [1,0]\n",
        "# Si c'est un rocher [0,1]\n",
        "import numpy as np\n",
        "n_labels = len(y)\n",
        "n_unique_labels = len(np.unique(y))\n",
        "one_hot_encode = np.zeros((n_labels, n_unique_labels))\n",
        "one_hot_encode[np.arange(n_labels), y] = 1\n",
        "Y = one_hot_encode"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjXwtQgGlVO1",
        "outputId": "1a37822d-3790-43ec-f248-56cb026f7078"
      },
      "source": [
        "#Verification en prenant les enregistrement 0 et 97\n",
        "print(\"Classe Rocher:\",int(Y[0][1]))\n",
        "print(\"Classe Mine :\",int(Y[97][1]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classe Rocher: 1\n",
            "Classe Mine : 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QCsELUQnI8g"
      },
      "source": [
        "# CREATION DES JEUX D'APPRENTISSAGE ET DE TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bwmn6G29nKWa"
      },
      "source": [
        "#On mélange\n",
        "from sklearn.utils import shuffle\n",
        "X, Y = shuffle(X, Y, random_state=1)\n",
        "\n",
        "#Creation des jeux d'apprentissage\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.07, random_state=42)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0u110xvnYDT"
      },
      "source": [
        "# PARAMETRAGE DU RESEAU DE  NEURONES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-dfS0mXnZa5"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "epochs = 300\n",
        "nombre_neurones_entree = 60\n",
        "nombre_neurones_sortie = 2\n",
        "taux_apprentissage = 0.01\n",
        "\n",
        "import tensorflow.compat.v1 as tf"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72Ioav59nx5t",
        "outputId": "d1bc81e6-0926-4c48-e135-7607c2ca879b"
      },
      "source": [
        "pip uninstall tensorflow"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.4.1:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow-2.4.1.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "y\n",
            "  Successfully uninstalled tensorflow-2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        },
        "id": "bRq3m6MzoOmk",
        "outputId": "088d4686-da70-4c37-c835-1ae38fb15ed2"
      },
      "source": [
        "pip install tensorflow"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/dc/e8c5e7983866fa4ef3fd619faa35f660b95b01a2ab62b3884f038ccab542/tensorflow-2.4.1-cp37-cp37m-manylinux2010_x86_64.whl (394.3MB)\n",
            "\u001b[K     |████████████████████████████████| 394.3MB 39kB/s \n",
            "\u001b[?25hRequirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.1)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.27.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (53.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Installing collected packages: tensorflow\n",
            "Successfully installed tensorflow-2.4.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 820
        },
        "id": "z2xyPh0-qlcq",
        "outputId": "2aaed9e2-c3aa-43e5-af19-d7682ed99f26"
      },
      "source": [
        "pip install tensorflow-gpu==1.14"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/67/559ca8408431c37ad3a17e859c8c291ea82f092354074baef482b98ffb7b/tensorflow_gpu-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (377.1MB)\n",
            "\u001b[K     |████████████████████████████████| 377.1MB 45kB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14) (0.36.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14) (0.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14) (1.19.5)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14) (1.12.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14) (1.32.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14) (0.10.0)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 43.0MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 47.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14) (0.8.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.14) (53.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (3.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (3.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (3.7.4.3)\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorboard~=2.4, but you'll have tensorboard 1.14.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorflow-estimator<2.5.0,>=2.4.0, but you'll have tensorflow-estimator 1.14.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-applications, tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "Successfully installed keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVihOpu2ngBH",
        "outputId": "fc2db09a-c0c9-429b-fa8d-003a5a12db0f"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "#Variable TensorFLow correspondant aux 60 valeurs des neurones d'entrée\n",
        "tf_neurones_entrees_X = tf.placeholder(tf.float32,[None, 60])\n",
        "\n",
        "#Variable TensorFlow correspondant au 2 neurones de sortie\n",
        "tf_valeurs_reelles_Y = tf.placeholder(tf.float32,[None, 2])\n",
        "\n",
        "\n",
        "poids = {\n",
        "    # 60 neurones d'entrées vers 24 Neurones de la couche cachée\n",
        "    'couche_entree_vers_cachee': tf.Variable(tf.random_uniform([60, 12], minval=-0.3, maxval=0.3), tf.float32),\n",
        "\n",
        "    # 12 neurones de la couche cachée vers 2 de la couche de sortie\n",
        "    'couche_cachee_vers_sortie': tf.Variable(tf.random_uniform([12, 2], minval=-0.3, maxval=0.3), tf.float32),\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "poids_biais = {\n",
        "     #1 biais de la couche d'entrée vers les 24 neurones de la couche cachée\n",
        "    'poids_biais_couche_entree_vers_cachee': tf.Variable(tf.zeros([12]), tf.float32),\n",
        "\n",
        "    #1 biais de la couche cachée vers les 2 neurones de la couche de sortie\n",
        "    'poids_biais_couche_cachee_vers_sortie': tf.Variable(tf.zeros([2]), tf.float32),\n",
        "}\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMTHp62ErYxQ"
      },
      "source": [
        "def reseau_neurones_multicouches(sonar_en_entrees, poids, poids_biais):\n",
        "\n",
        "    #Calcul de l'activation de la première couche\n",
        "    premiere_activation = tf.sigmoid(tf.matmul(tf_neurones_entrees_X, poids['couche_entree_vers_cachee']) + poids_biais['poids_biais_couche_entree_vers_cachee'])\n",
        "\n",
        "    #Calcul de l'activation de la seconde couche\n",
        "    activation_couche_cachee = tf.sigmoid(tf.matmul(premiere_activation, poids['couche_cachee_vers_sortie']) + poids_biais['poids_biais_couche_cachee_vers_sortie'])\n",
        "\n",
        "    return activation_couche_cachee"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOn7PNvyraJ5"
      },
      "source": [
        "reseau = reseau_neurones_multicouches(tf_neurones_entrees_X, poids, poids_biais)\n",
        "\n",
        "\n",
        "#---------------------------------------------\n",
        "# ERREUR ET OPTIMISATION\n",
        "#---------------------------------------------\n",
        "\n",
        "#Fonction d'erreur de moyenne quadratique MSE\n",
        "fonction_erreur = tf.reduce_sum(tf.pow(tf_valeurs_reelles_Y-reseau,2))\n",
        "\n",
        "#Fonction de precision\n",
        "fonction_precision = tf.metrics.accuracy(labels=tf_valeurs_reelles_Y,predictions=reseau)\n",
        "\n",
        "\n",
        "#Descente de gradient avec un taux d'apprentissage fixé à 0.1\n",
        "optimiseur = tf.train.GradientDescentOptimizer(learning_rate=taux_apprentissage).minimize(fonction_erreur)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPsm5r2lsHcF"
      },
      "source": [
        " CREATION DU RESEAU DE NEURONES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX0pRrXaruEr"
      },
      "source": [
        "reseau = reseau_neurones_multicouches(tf_neurones_entrees_X, poids, poids_biais)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzPi02fRsOOb"
      },
      "source": [
        "ERREUR ET OPTIMISATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mQYWRc3sKai"
      },
      "source": [
        "#Fonction d'erreur de moyenne quadratique MSE\n",
        "fonction_erreur = tf.reduce_sum(tf.pow(tf_valeurs_reelles_Y-reseau,2))\n",
        "\n",
        "#Fonction de precision\n",
        "fonction_precision = tf.metrics.accuracy(labels=tf_valeurs_reelles_Y,predictions=reseau)\n",
        "\n",
        "\n",
        "#Descente de gradient avec un taux d'apprentissage fixé à 0.1\n",
        "optimiseur = tf.train.GradientDescentOptimizer(learning_rate=taux_apprentissage).minimize(fonction_erreur)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCyEhboAsUI3"
      },
      "source": [
        "# APPRENTISSAGE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3AE0KDRdsSkG",
        "outputId": "ccddc2e5-fffa-4dae-9ab4-d70adf9445db"
      },
      "source": [
        "#Initialisation des variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "#Demarrage d'une session d'apprentissage\n",
        "session = tf.Session()\n",
        "session.run(init)\n",
        "\n",
        "#Pour la réalisation du graphique pour la MSE\n",
        "Graphique_MSE=[]\n",
        "\n",
        "\n",
        "#Pour chaque epoch\n",
        "for i in range(epochs):\n",
        "\n",
        "   #Realisation de l'apprentissage avec mise à jour des poids\n",
        "   session.run(optimiseur, feed_dict = {tf_neurones_entrees_X: train_x, tf_valeurs_reelles_Y:train_y})\n",
        "\n",
        "   #Calculer l'erreur d'apprentissage\n",
        "   MSE = session.run(fonction_erreur, feed_dict = {tf_neurones_entrees_X: train_x, tf_valeurs_reelles_Y:train_y})\n",
        "\n",
        "   #Affichage des informations\n",
        "   Graphique_MSE.append(MSE)\n",
        "   print(\"EPOCH (\" + str(i) + \"/\" + str(epochs) + \") -  MSE: \"+ str(MSE))\n",
        "\n",
        "\n",
        "#Affichage graphique MSE\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(Graphique_MSE)\n",
        "plt.ylabel('MSE')\n",
        "plt.show()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH (0/300) -  MSE: 95.27215\n",
            "EPOCH (1/300) -  MSE: 95.15721\n",
            "EPOCH (2/300) -  MSE: 95.05417\n",
            "EPOCH (3/300) -  MSE: 94.95063\n",
            "EPOCH (4/300) -  MSE: 94.84611\n",
            "EPOCH (5/300) -  MSE: 94.74013\n",
            "EPOCH (6/300) -  MSE: 94.63228\n",
            "EPOCH (7/300) -  MSE: 94.52211\n",
            "EPOCH (8/300) -  MSE: 94.40921\n",
            "EPOCH (9/300) -  MSE: 94.29315\n",
            "EPOCH (10/300) -  MSE: 94.1735\n",
            "EPOCH (11/300) -  MSE: 94.04984\n",
            "EPOCH (12/300) -  MSE: 93.92177\n",
            "EPOCH (13/300) -  MSE: 93.78883\n",
            "EPOCH (14/300) -  MSE: 93.65064\n",
            "EPOCH (15/300) -  MSE: 93.506775\n",
            "EPOCH (16/300) -  MSE: 93.3568\n",
            "EPOCH (17/300) -  MSE: 93.20028\n",
            "EPOCH (18/300) -  MSE: 93.03683\n",
            "EPOCH (19/300) -  MSE: 92.86601\n",
            "EPOCH (20/300) -  MSE: 92.6874\n",
            "EPOCH (21/300) -  MSE: 92.50059\n",
            "EPOCH (22/300) -  MSE: 92.305145\n",
            "EPOCH (23/300) -  MSE: 92.10069\n",
            "EPOCH (24/300) -  MSE: 91.8868\n",
            "EPOCH (25/300) -  MSE: 91.6631\n",
            "EPOCH (26/300) -  MSE: 91.42919\n",
            "EPOCH (27/300) -  MSE: 91.18471\n",
            "EPOCH (28/300) -  MSE: 90.9293\n",
            "EPOCH (29/300) -  MSE: 90.66263\n",
            "EPOCH (30/300) -  MSE: 90.384384\n",
            "EPOCH (31/300) -  MSE: 90.0943\n",
            "EPOCH (32/300) -  MSE: 89.792076\n",
            "EPOCH (33/300) -  MSE: 89.47753\n",
            "EPOCH (34/300) -  MSE: 89.15048\n",
            "EPOCH (35/300) -  MSE: 88.810776\n",
            "EPOCH (36/300) -  MSE: 88.45832\n",
            "EPOCH (37/300) -  MSE: 88.09307\n",
            "EPOCH (38/300) -  MSE: 87.71501\n",
            "EPOCH (39/300) -  MSE: 87.32422\n",
            "EPOCH (40/300) -  MSE: 86.920784\n",
            "EPOCH (41/300) -  MSE: 86.50489\n",
            "EPOCH (42/300) -  MSE: 86.07672\n",
            "EPOCH (43/300) -  MSE: 85.63658\n",
            "EPOCH (44/300) -  MSE: 85.18476\n",
            "EPOCH (45/300) -  MSE: 84.72166\n",
            "EPOCH (46/300) -  MSE: 84.24768\n",
            "EPOCH (47/300) -  MSE: 83.763275\n",
            "EPOCH (48/300) -  MSE: 83.26897\n",
            "EPOCH (49/300) -  MSE: 82.76528\n",
            "EPOCH (50/300) -  MSE: 82.2528\n",
            "EPOCH (51/300) -  MSE: 81.73212\n",
            "EPOCH (52/300) -  MSE: 81.203896\n",
            "EPOCH (53/300) -  MSE: 80.668755\n",
            "EPOCH (54/300) -  MSE: 80.127396\n",
            "EPOCH (55/300) -  MSE: 79.58052\n",
            "EPOCH (56/300) -  MSE: 79.028824\n",
            "EPOCH (57/300) -  MSE: 78.47303\n",
            "EPOCH (58/300) -  MSE: 77.91389\n",
            "EPOCH (59/300) -  MSE: 77.35213\n",
            "EPOCH (60/300) -  MSE: 76.788506\n",
            "EPOCH (61/300) -  MSE: 76.22376\n",
            "EPOCH (62/300) -  MSE: 75.658646\n",
            "EPOCH (63/300) -  MSE: 75.0939\n",
            "EPOCH (64/300) -  MSE: 74.530266\n",
            "EPOCH (65/300) -  MSE: 73.96845\n",
            "EPOCH (66/300) -  MSE: 73.409164\n",
            "EPOCH (67/300) -  MSE: 72.85309\n",
            "EPOCH (68/300) -  MSE: 72.30088\n",
            "EPOCH (69/300) -  MSE: 71.75318\n",
            "EPOCH (70/300) -  MSE: 71.21057\n",
            "EPOCH (71/300) -  MSE: 70.673615\n",
            "EPOCH (72/300) -  MSE: 70.14284\n",
            "EPOCH (73/300) -  MSE: 69.61873\n",
            "EPOCH (74/300) -  MSE: 69.10174\n",
            "EPOCH (75/300) -  MSE: 68.592224\n",
            "EPOCH (76/300) -  MSE: 68.09056\n",
            "EPOCH (77/300) -  MSE: 67.59705\n",
            "EPOCH (78/300) -  MSE: 67.11196\n",
            "EPOCH (79/300) -  MSE: 66.63549\n",
            "EPOCH (80/300) -  MSE: 66.16782\n",
            "EPOCH (81/300) -  MSE: 65.70909\n",
            "EPOCH (82/300) -  MSE: 65.259384\n",
            "EPOCH (83/300) -  MSE: 64.81875\n",
            "EPOCH (84/300) -  MSE: 64.38723\n",
            "EPOCH (85/300) -  MSE: 63.96481\n",
            "EPOCH (86/300) -  MSE: 63.551453\n",
            "EPOCH (87/300) -  MSE: 63.1471\n",
            "EPOCH (88/300) -  MSE: 62.751667\n",
            "EPOCH (89/300) -  MSE: 62.36505\n",
            "EPOCH (90/300) -  MSE: 61.987137\n",
            "EPOCH (91/300) -  MSE: 61.617786\n",
            "EPOCH (92/300) -  MSE: 61.25685\n",
            "EPOCH (93/300) -  MSE: 60.90421\n",
            "EPOCH (94/300) -  MSE: 60.559727\n",
            "EPOCH (95/300) -  MSE: 60.22347\n",
            "EPOCH (96/300) -  MSE: 59.896126\n",
            "EPOCH (97/300) -  MSE: 59.580975\n",
            "EPOCH (98/300) -  MSE: 59.291714\n",
            "EPOCH (99/300) -  MSE: 59.085964\n",
            "EPOCH (100/300) -  MSE: 59.19592\n",
            "EPOCH (101/300) -  MSE: 60.612156\n",
            "EPOCH (102/300) -  MSE: 66.380585\n",
            "EPOCH (103/300) -  MSE: 84.117226\n",
            "EPOCH (104/300) -  MSE: 86.908745\n",
            "EPOCH (105/300) -  MSE: 102.66056\n",
            "EPOCH (106/300) -  MSE: 65.35905\n",
            "EPOCH (107/300) -  MSE: 77.42258\n",
            "EPOCH (108/300) -  MSE: 79.356735\n",
            "EPOCH (109/300) -  MSE: 92.69797\n",
            "EPOCH (110/300) -  MSE: 67.70898\n",
            "EPOCH (111/300) -  MSE: 77.56\n",
            "EPOCH (112/300) -  MSE: 72.84077\n",
            "EPOCH (113/300) -  MSE: 82.24119\n",
            "EPOCH (114/300) -  MSE: 69.049286\n",
            "EPOCH (115/300) -  MSE: 76.233215\n",
            "EPOCH (116/300) -  MSE: 69.01297\n",
            "EPOCH (117/300) -  MSE: 75.195656\n",
            "EPOCH (118/300) -  MSE: 67.90132\n",
            "EPOCH (119/300) -  MSE: 73.161354\n",
            "EPOCH (120/300) -  MSE: 67.10027\n",
            "EPOCH (121/300) -  MSE: 71.78714\n",
            "EPOCH (122/300) -  MSE: 66.40134\n",
            "EPOCH (123/300) -  MSE: 70.70465\n",
            "EPOCH (124/300) -  MSE: 65.79982\n",
            "EPOCH (125/300) -  MSE: 69.841156\n",
            "EPOCH (126/300) -  MSE: 65.27243\n",
            "EPOCH (127/300) -  MSE: 69.12484\n",
            "EPOCH (128/300) -  MSE: 64.797325\n",
            "EPOCH (129/300) -  MSE: 68.503525\n",
            "EPOCH (130/300) -  MSE: 64.35779\n",
            "EPOCH (131/300) -  MSE: 67.94289\n",
            "EPOCH (132/300) -  MSE: 63.94262\n",
            "EPOCH (133/300) -  MSE: 67.42195\n",
            "EPOCH (134/300) -  MSE: 63.5449\n",
            "EPOCH (135/300) -  MSE: 66.92864\n",
            "EPOCH (136/300) -  MSE: 63.16061\n",
            "EPOCH (137/300) -  MSE: 66.45627\n",
            "EPOCH (138/300) -  MSE: 62.787487\n",
            "EPOCH (139/300) -  MSE: 66.00112\n",
            "EPOCH (140/300) -  MSE: 62.42423\n",
            "EPOCH (141/300) -  MSE: 65.56111\n",
            "EPOCH (142/300) -  MSE: 62.070053\n",
            "EPOCH (143/300) -  MSE: 65.134926\n",
            "EPOCH (144/300) -  MSE: 61.724438\n",
            "EPOCH (145/300) -  MSE: 64.7217\n",
            "EPOCH (146/300) -  MSE: 61.386997\n",
            "EPOCH (147/300) -  MSE: 64.32075\n",
            "EPOCH (148/300) -  MSE: 61.057438\n",
            "EPOCH (149/300) -  MSE: 63.93151\n",
            "EPOCH (150/300) -  MSE: 60.735477\n",
            "EPOCH (151/300) -  MSE: 63.553486\n",
            "EPOCH (152/300) -  MSE: 60.42088\n",
            "EPOCH (153/300) -  MSE: 63.18623\n",
            "EPOCH (154/300) -  MSE: 60.113415\n",
            "EPOCH (155/300) -  MSE: 62.82931\n",
            "EPOCH (156/300) -  MSE: 59.812866\n",
            "EPOCH (157/300) -  MSE: 62.482334\n",
            "EPOCH (158/300) -  MSE: 59.519035\n",
            "EPOCH (159/300) -  MSE: 62.144917\n",
            "EPOCH (160/300) -  MSE: 59.23172\n",
            "EPOCH (161/300) -  MSE: 61.816696\n",
            "EPOCH (162/300) -  MSE: 58.950726\n",
            "EPOCH (163/300) -  MSE: 61.497322\n",
            "EPOCH (164/300) -  MSE: 58.67587\n",
            "EPOCH (165/300) -  MSE: 61.186478\n",
            "EPOCH (166/300) -  MSE: 58.40696\n",
            "EPOCH (167/300) -  MSE: 60.883816\n",
            "EPOCH (168/300) -  MSE: 58.14382\n",
            "EPOCH (169/300) -  MSE: 60.58905\n",
            "EPOCH (170/300) -  MSE: 57.886288\n",
            "EPOCH (171/300) -  MSE: 60.301876\n",
            "EPOCH (172/300) -  MSE: 57.634182\n",
            "EPOCH (173/300) -  MSE: 60.021996\n",
            "EPOCH (174/300) -  MSE: 57.387337\n",
            "EPOCH (175/300) -  MSE: 59.74913\n",
            "EPOCH (176/300) -  MSE: 57.145584\n",
            "EPOCH (177/300) -  MSE: 59.483025\n",
            "EPOCH (178/300) -  MSE: 56.908775\n",
            "EPOCH (179/300) -  MSE: 59.22341\n",
            "EPOCH (180/300) -  MSE: 56.676754\n",
            "EPOCH (181/300) -  MSE: 58.97006\n",
            "EPOCH (182/300) -  MSE: 56.449364\n",
            "EPOCH (183/300) -  MSE: 58.722706\n",
            "EPOCH (184/300) -  MSE: 56.226463\n",
            "EPOCH (185/300) -  MSE: 58.48114\n",
            "EPOCH (186/300) -  MSE: 56.007908\n",
            "EPOCH (187/300) -  MSE: 58.24514\n",
            "EPOCH (188/300) -  MSE: 55.793568\n",
            "EPOCH (189/300) -  MSE: 58.01449\n",
            "EPOCH (190/300) -  MSE: 55.583298\n",
            "EPOCH (191/300) -  MSE: 57.78899\n",
            "EPOCH (192/300) -  MSE: 55.37697\n",
            "EPOCH (193/300) -  MSE: 57.56844\n",
            "EPOCH (194/300) -  MSE: 55.17447\n",
            "EPOCH (195/300) -  MSE: 57.352665\n",
            "EPOCH (196/300) -  MSE: 54.97567\n",
            "EPOCH (197/300) -  MSE: 57.1415\n",
            "EPOCH (198/300) -  MSE: 54.78046\n",
            "EPOCH (199/300) -  MSE: 56.934765\n",
            "EPOCH (200/300) -  MSE: 54.588737\n",
            "EPOCH (201/300) -  MSE: 56.7323\n",
            "EPOCH (202/300) -  MSE: 54.400375\n",
            "EPOCH (203/300) -  MSE: 56.53395\n",
            "EPOCH (204/300) -  MSE: 54.215294\n",
            "EPOCH (205/300) -  MSE: 56.33958\n",
            "EPOCH (206/300) -  MSE: 54.03338\n",
            "EPOCH (207/300) -  MSE: 56.149048\n",
            "EPOCH (208/300) -  MSE: 53.854546\n",
            "EPOCH (209/300) -  MSE: 55.962227\n",
            "EPOCH (210/300) -  MSE: 53.678696\n",
            "EPOCH (211/300) -  MSE: 55.77898\n",
            "EPOCH (212/300) -  MSE: 53.505753\n",
            "EPOCH (213/300) -  MSE: 55.599197\n",
            "EPOCH (214/300) -  MSE: 53.335625\n",
            "EPOCH (215/300) -  MSE: 55.422768\n",
            "EPOCH (216/300) -  MSE: 53.16824\n",
            "EPOCH (217/300) -  MSE: 55.249577\n",
            "EPOCH (218/300) -  MSE: 53.00352\n",
            "EPOCH (219/300) -  MSE: 55.079533\n",
            "EPOCH (220/300) -  MSE: 52.84139\n",
            "EPOCH (221/300) -  MSE: 54.912514\n",
            "EPOCH (222/300) -  MSE: 52.68177\n",
            "EPOCH (223/300) -  MSE: 54.748455\n",
            "EPOCH (224/300) -  MSE: 52.524605\n",
            "EPOCH (225/300) -  MSE: 54.58725\n",
            "EPOCH (226/300) -  MSE: 52.369823\n",
            "EPOCH (227/300) -  MSE: 54.42882\n",
            "EPOCH (228/300) -  MSE: 52.217373\n",
            "EPOCH (229/300) -  MSE: 54.273075\n",
            "EPOCH (230/300) -  MSE: 52.067165\n",
            "EPOCH (231/300) -  MSE: 54.119953\n",
            "EPOCH (232/300) -  MSE: 51.919178\n",
            "EPOCH (233/300) -  MSE: 53.96936\n",
            "EPOCH (234/300) -  MSE: 51.77333\n",
            "EPOCH (235/300) -  MSE: 53.821236\n",
            "EPOCH (236/300) -  MSE: 51.629574\n",
            "EPOCH (237/300) -  MSE: 53.67551\n",
            "EPOCH (238/300) -  MSE: 51.487858\n",
            "EPOCH (239/300) -  MSE: 53.532116\n",
            "EPOCH (240/300) -  MSE: 51.348125\n",
            "EPOCH (241/300) -  MSE: 53.39099\n",
            "EPOCH (242/300) -  MSE: 51.210342\n",
            "EPOCH (243/300) -  MSE: 53.25207\n",
            "EPOCH (244/300) -  MSE: 51.07444\n",
            "EPOCH (245/300) -  MSE: 53.115303\n",
            "EPOCH (246/300) -  MSE: 50.940388\n",
            "EPOCH (247/300) -  MSE: 52.980625\n",
            "EPOCH (248/300) -  MSE: 50.808132\n",
            "EPOCH (249/300) -  MSE: 52.847984\n",
            "EPOCH (250/300) -  MSE: 50.677628\n",
            "EPOCH (251/300) -  MSE: 52.71733\n",
            "EPOCH (252/300) -  MSE: 50.548847\n",
            "EPOCH (253/300) -  MSE: 52.588615\n",
            "EPOCH (254/300) -  MSE: 50.42174\n",
            "EPOCH (255/300) -  MSE: 52.461784\n",
            "EPOCH (256/300) -  MSE: 50.296253\n",
            "EPOCH (257/300) -  MSE: 52.336792\n",
            "EPOCH (258/300) -  MSE: 50.17237\n",
            "EPOCH (259/300) -  MSE: 52.213593\n",
            "EPOCH (260/300) -  MSE: 50.050045\n",
            "EPOCH (261/300) -  MSE: 52.09214\n",
            "EPOCH (262/300) -  MSE: 49.92923\n",
            "EPOCH (263/300) -  MSE: 51.972393\n",
            "EPOCH (264/300) -  MSE: 49.809906\n",
            "EPOCH (265/300) -  MSE: 51.854313\n",
            "EPOCH (266/300) -  MSE: 49.692028\n",
            "EPOCH (267/300) -  MSE: 51.737846\n",
            "EPOCH (268/300) -  MSE: 49.57556\n",
            "EPOCH (269/300) -  MSE: 51.62298\n",
            "EPOCH (270/300) -  MSE: 49.460476\n",
            "EPOCH (271/300) -  MSE: 51.509644\n",
            "EPOCH (272/300) -  MSE: 49.346745\n",
            "EPOCH (273/300) -  MSE: 51.397816\n",
            "EPOCH (274/300) -  MSE: 49.23433\n",
            "EPOCH (275/300) -  MSE: 51.28748\n",
            "EPOCH (276/300) -  MSE: 49.123203\n",
            "EPOCH (277/300) -  MSE: 51.17857\n",
            "EPOCH (278/300) -  MSE: 49.013332\n",
            "EPOCH (279/300) -  MSE: 51.071075\n",
            "EPOCH (280/300) -  MSE: 48.904694\n",
            "EPOCH (281/300) -  MSE: 50.964943\n",
            "EPOCH (282/300) -  MSE: 48.79725\n",
            "EPOCH (283/300) -  MSE: 50.86016\n",
            "EPOCH (284/300) -  MSE: 48.69098\n",
            "EPOCH (285/300) -  MSE: 50.75668\n",
            "EPOCH (286/300) -  MSE: 48.585857\n",
            "EPOCH (287/300) -  MSE: 50.65449\n",
            "EPOCH (288/300) -  MSE: 48.481853\n",
            "EPOCH (289/300) -  MSE: 50.553547\n",
            "EPOCH (290/300) -  MSE: 48.378944\n",
            "EPOCH (291/300) -  MSE: 50.453823\n",
            "EPOCH (292/300) -  MSE: 48.2771\n",
            "EPOCH (293/300) -  MSE: 50.355305\n",
            "EPOCH (294/300) -  MSE: 48.176308\n",
            "EPOCH (295/300) -  MSE: 50.257954\n",
            "EPOCH (296/300) -  MSE: 48.07653\n",
            "EPOCH (297/300) -  MSE: 50.161747\n",
            "EPOCH (298/300) -  MSE: 47.977757\n",
            "EPOCH (299/300) -  MSE: 50.066658\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQQ5ChhMsiWb"
      },
      "source": [
        "# VERIFICATION DE L'APPRENTISSAGE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRn7837-sfXO"
      },
      "source": [
        "#Les probabilités de chaque classe 'Mine' ou 'rocher' issues de l'apprentissage sont stockée dans le modèle.\n",
        "#A l'aide de tf.argmax, on récupére les indexs des probabilités les plus elevées pour chaque sonar\n",
        "#Ex: Si pour une observation nous avons [0.56, 0.89] renverra 1 car la valeur la plus élevée se trouve à l'index 1\n",
        "#Ex : Si pour une observation nous avons [0.90, 0.34 ]  renverra 0 car la valeur la plus élevée se trouve à l'index 0\n",
        "classifications = tf.argmax(reseau, 1)\n",
        "\n",
        "#Dans le tableau des valeurs réelles :\n",
        "#Les mines sont encodées comme suit [1,0] l'index ayant la plus grande valeur est 0\n",
        "#Les rochers ont pour valeur [0,1] sl'index ayant la plus grande valeur est 1\n",
        "\n",
        "#Si la classification est de [0.90, 0.34 ] l'index ayant la plus grande valeur est 0\n",
        "#Si c'est une mine [1,0] l'index ayant la plus grande valeur est 0\n",
        "#Si les deux index sont identiques alors on peut affirmer que c'est une bonne classification\n",
        "formule_calcul_bonnes_classifications = tf.equal(classifications, tf.argmax(tf_valeurs_reelles_Y,1))\n",
        "\n",
        "\n",
        "#La précision se calcul en faisant la moyenne (tf.mean)\n",
        "# des bonnes classifications (aprés les avoir converties en décimale tf.cast, tf.float32)\n",
        "formule_precision = tf.reduce_mean(tf.cast(formule_calcul_bonnes_classifications, tf.float32))\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RodAhMnzssZh"
      },
      "source": [
        "# PRECISION SUR LES DONNEES DE TESTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sheiJAhPson5",
        "outputId": "a89f76d8-531a-45cc-bb92-5dacff483553"
      },
      "source": [
        "nb_classifications = 0;\n",
        "nb_bonnes_classifications = 0\n",
        "\n",
        "#On parcours l'ensemble des données de test (text_x)\n",
        "for i in range(0,test_x.shape[0]):\n",
        "\n",
        "    #On récupere les informations\n",
        "    donneesSonar = test_x[i].reshape(1,60)\n",
        "    classificationAttendue = test_y[i].reshape(1,2)\n",
        "\n",
        "    # On réalise la classification\n",
        "    prediction_run = session.run(classifications, feed_dict={tf_neurones_entrees_X:donneesSonar})\n",
        "\n",
        "    #On calcule la précision de la classification à l'aide de la formule établie auparavant\n",
        "    accuracy_run = session.run(formule_precision, feed_dict={tf_neurones_entrees_X:donneesSonar, tf_valeurs_reelles_Y:classificationAttendue})\n",
        "\n",
        "\n",
        "    #On affiche pour observation la classe originale et la classification réalisée\n",
        "    print(i,\"Classe attendue: \", int(session.run(tf_valeurs_reelles_Y[i][1],feed_dict={tf_valeurs_reelles_Y:test_y})), \" Classification: \", prediction_run[0] )\n",
        "\n",
        "    nb_classifications = nb_classifications+1\n",
        "    if(accuracy_run*100 ==100):\n",
        "        nb_bonnes_classifications = nb_bonnes_classifications+1\n",
        "\n",
        "\n",
        "print(\"-------------\")\n",
        "print(\"Précision sur les donnees de tests = \"+str((nb_bonnes_classifications/nb_classifications)*100)+\"%\")\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------\n",
        "# PRECISION SUR LES DONNEES D'APPRENTISSAGE\n",
        "#-------------------------------------------------------------------------\n",
        "\n",
        "nb_classifications = 0;\n",
        "nb_bonnes_classifications = 0\n",
        "for i in range(0,train_x.shape[0]):\n",
        "\n",
        "    # On récupere les informations\n",
        "    donneesSonar = train_x[i].reshape(1, 60)\n",
        "    classificationAttendue = train_y[i].reshape(1, 2)\n",
        "\n",
        "    # On réalise la classification\n",
        "    prediction_run = session.run(classifications, feed_dict={tf_neurones_entrees_X: donneesSonar})\n",
        "\n",
        "    # On calcule la précision de la classification à l'aide de la formule établie auparavant\n",
        "    accuracy_run = session.run(formule_precision, feed_dict={tf_neurones_entrees_X: donneesSonar, tf_valeurs_reelles_Y: classificationAttendue})\n",
        "\n",
        "    nb_classifications = nb_classifications + 1\n",
        "    if (accuracy_run * 100 == 100):\n",
        "        nb_bonnes_classifications = nb_bonnes_classifications + 1\n",
        "\n",
        "\n",
        "print(\"Précision sur les donnees d'apprentissage = \" + str((nb_bonnes_classifications / nb_classifications) * 100) + \"%\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Classe attendue:  0  Classification:  0\n",
            "1 Classe attendue:  1  Classification:  0\n",
            "2 Classe attendue:  1  Classification:  0\n",
            "3 Classe attendue:  0  Classification:  0\n",
            "4 Classe attendue:  1  Classification:  0\n",
            "5 Classe attendue:  1  Classification:  0\n",
            "6 Classe attendue:  1  Classification:  0\n",
            "7 Classe attendue:  1  Classification:  0\n",
            "8 Classe attendue:  1  Classification:  0\n",
            "9 Classe attendue:  1  Classification:  1\n",
            "10 Classe attendue:  0  Classification:  0\n",
            "11 Classe attendue:  1  Classification:  1\n",
            "12 Classe attendue:  0  Classification:  0\n",
            "13 Classe attendue:  1  Classification:  1\n",
            "14 Classe attendue:  1  Classification:  0\n",
            "-------------\n",
            "Précision sur les donnees de tests = 46.666666666666664%\n",
            "Précision sur les donnees d'apprentissage = 81.86528497409327%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3yle4ANsyhJ"
      },
      "source": [
        "# PRECISION SUR L'ENSEMBLE DES DONNEES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DQnVtYYswpQ",
        "outputId": "b90cd74f-a24f-46d8-d2da-bf2a16318cf2"
      },
      "source": [
        "nb_classifications = 0;\n",
        "nb_bonnes_classifications = 0\n",
        "for i in range(0,207):\n",
        "\n",
        "    prediction_run = session.run(classifications, feed_dict={tf_neurones_entrees_X:X[i].reshape(1,60)})\n",
        "    accuracy_run = session.run(formule_precision, feed_dict={tf_neurones_entrees_X:X[i].reshape(1,60), tf_valeurs_reelles_Y:Y[i].reshape(1,2)})\n",
        "\n",
        "    nb_classifications = nb_classifications + 1\n",
        "    if (accuracy_run * 100 == 100):\n",
        "        nb_bonnes_classifications = nb_bonnes_classifications + 1\n",
        "\n",
        "\n",
        "print(\"Précision sur l'ensemble des données = \" + str((nb_bonnes_classifications / nb_classifications) * 100) + \"%\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "session.close()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Précision sur l'ensemble des données = 79.22705314009661%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}